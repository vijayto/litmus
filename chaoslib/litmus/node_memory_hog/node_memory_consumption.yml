 ## GETTING THE PRE CHAOS NODE NAME OF APPLICATION 
- block:
  
    - name: Getting the pre-chaos application pod name 
      k8s_facts:
        kind: Pod
        namespace: "{{ a_ns }}"
        label_selectors:
          - "{{ a_label }}"
      register: app_pod_name
    
    ## RECORDING THE NODE NAME ON WHICH APPLICATION IS RUNNING
    - name: Recording the node name of application pod
      set_fact:
        node_name: "{{ app_pod_name | json_query('resources[0].spec.nodeName')}}"
        
    - block:
        - name: Generate a run id if not passed from the engine/experiment
          shell: echo $(mktemp) | cut -d '.' -f 2 | cut -c -6
          register: rand_string   

        - set_fact:
            run_id: "{{ rand_string.stdout | lower }}"
      when: "run_id is not defined or run_id == ''"
  
    ## RECORD EVENT FOR CHAOS INJECTION
    - include_tasks: /utils/common/generate-kubernetes-chaos-events.yml
      vars:
        stage: "ChaosInject"
        exp_pod_name: "{{ chaos_pod_name }}"
        engine_ns: "{{ c_ns }}"
        message: "Injecting {{ c_experiment }} chaos on {{ node_name }} node"
      when: "c_engine is defined and c_engine != ''"
    
    ## PATCH THE ENGINE UID
    - name: Patch the engine uid
      template:
        src:  /chaoslib/litmus/node_memory_hog/node-memory-hog-pod.j2
        dest: /chaoslib/litmus/node_memory_hog/node-memory-hog-pod.yml
      vars:
        stress_time: "{{ c_duration }}s"
        total_memory_percentage: "{{ memory_percentage }}%"
        memory_stress_image: "{{ lib_image }}"

    ## WAIT FOR THE SPECIFIED RAMP TIME
    - name: Wait for the specified ramp time before injecting chaos
      wait_for: timeout="{{ ramp_time }}"
      when: "ramp_time is defined and ramp_time != ''"

    # Setting pod_running_status to nil
    - set_fact: 
        pod_running_status: ""
        
     # Node Memory Hog pods creation is attempted for a total of 3 times, if it is not immediately schedulable due to transient node conditions
     # If the node-memory-hog pod is not schedulable across these 3 tries, the experiment is failed with message indicating improper cluster state.
    - include_tasks: /utils/common/create_chaos_pod.yml
      vars:
        pod_ns: "{{ c_ns }}"
        c_path: "/chaoslib/litmus/node_memory_hog/node-memory-hog-pod.yml"
        pod_label: "name=node-memory-hog-{{ run_id }}"
      with_sequence: start=1 end=3

    # Failing the execution, If node-memory-hog pod won't come to running state after three retries.
    - fail:
        msg: "node_memory_consumption lib failed, Unable to create as node-memory-hog pod couldn't be scheduled on the {{ node_name }} node"
      when: "pod_running_status is not defined or pod_running_status != 'Running'"

    ## CHECKING THE STATUS OF JOB
    - name: Checking the status of Job
      k8s_facts:
        kind: Pod
        namespace: "{{ c_ns }}"
        label_selectors:
          - name=node-memory-hog-{{ run_id }}
      register: job_pod
      until: "{{ job_pod | json_query('resources[0].status.phase') == 'Succeeded' }}"
      delay: 1
      retries: "{{ c_duration | int  + 60 }}"
    
    ## GETTING THE JOB POD LOGS UNDER STRESS
    - name: Getting the pod logs
      shell: kubectl logs {{ job_pod.resources[0].metadata.name }} -n {{ c_ns }}
      args:
        executable: /bin/bash

    ## WAIT FOR RAMP TIME AFTER CHAOS
    - name: Wait for the specified ramp time after injecting chaos
      wait_for: timeout="{{ ramp_time }}"
      when: "ramp_time is defined and ramp_time != ''" 

    ## GETTING THE NODE STATUS
    - name: Getting the node status
      shell: kubectl get nodes {{ node_name }} | awk '{print $2}' | tail -n1
      register: node_status

    - block:

        ## DELETING THE POD
        - name: Deleting the Memory Hog Pod
          shell: kubectl delete -f /chaoslib/litmus/node_memory_hog/node-memory-hog-pod.yml -n {{ c_ns }}
          args:
            executable: /bin/bash
    
        ## CHECKING WHETHER MEMORY HOG POD IS DELETED SUCCESSFULLY
        - name: Confirm that the Memory Hog Pod is deleted successfully
          k8s_facts:
            kind: Pod
            namespace: "{{ c_ns }}"
            label_selectors:
              - name=node-memory-hog-{{ run_id }}
          register: resource_pod
          until: "resource_pod.resources | length < 1"
          delay: 2
          retries: 90

      when: "pod_running_status is defined and pod_running_status == 'Running'"

  rescue: 

    - block: 

        ## REMOVING THE MEMORY HOG Pod
        - name: Deleting the Memory Hog Pod
          shell: >
            kubectl delete -f /chaoslib/litmus/node_memory_hog/node-memory-hog-pod.yml -n {{ c_ns }}
          args:
            executable: /bin/bash
          when: "chaos_pod_result.rc == 0 "
        
        ## CHECKING WHETHER MEMORY HOG POD IS DELETED SUCCESSFULLY
        - name: Confirm that the memory Hog Pod is deleted successfully
          k8s_facts:
            kind: Pod
            namespace: "{{ c_ns }}"
            label_selectors:
              - name=node-memory-hog-{{ run_id }}
          register: resource_pod
          until: "resource_pod.resources | length < 1"
          delay: 2
          retries: 90
      when: "(pod_running_status is defined and pod_running_status == 'Running') and chaos_pod_result is defined"

    ## FAIL WHEN ENTERS IN RESCUE BLOCK
    - name: Fail the task if it enters in the rescue block
      fail:
        msg: "node_memory_consumption lib failed"
      when: true
      